{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907ca8a8-c16a-43f2-8ff7-56278cfda374",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "In this notebook we use a model that can summarize texts. We will summarize the first two sections of the following paper from OpenAI about a model named clip:\n",
    "\n",
    "CLIP Paper: https://arxiv.org/abs/2103.00020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3354c-dd97-4621-ba60-2f85e60576fe",
   "metadata": {},
   "source": [
    "Model (1.22 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4db1767-f4e7-498d-aebb-acfeab66bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_text(text):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    summary = summarizer(text, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "    return summary\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def summarize_text_file(file_path):\n",
    "    # Read the content of the text file\n",
    "    text_content = read_text_file(file_path)\n",
    "\n",
    "    # Generate a summary of the text content\n",
    "    return summarize_text(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f679e-2497-4f6a-a2d9-d1fd2fda780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_abstract = summarize_text_file(\"./texts/abstract.txt\")\n",
    "summary_intro1 = summarize_text_file(\"./texts/introduction.txt\")\n",
    "summary_intro2 = summarize_text_file(\"./texts/introduction2.txt\")\n",
    "summary_approach21 = summarize_text_file(\"./texts/approach21.txt\")\n",
    "summary_approach22 = summarize_text_file(\"./texts/approach22.txt\")\n",
    "summary_approach23 = summarize_text_file(\"./texts/approach23.txt\")\n",
    "summary_approach24 = summarize_text_file(\"./texts/approach24.txt\")\n",
    "summary_approach25 = summarize_text_file(\"./texts/approach25.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9908b570-b727-40d3-9be6-14074a12a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"section\": [\n",
    "        \"Abstract\",\n",
    "        \"1 Introduction\",\n",
    "        \"2.1 Natural Language Supervision\",\n",
    "        \"2.2 Creating a Sufficiently Large Dataset\",\n",
    "        \"2.3 Selecting an Efficient Pre-Training Method\",\n",
    "        \"2.4 Choosing and Scaling a Model\",\n",
    "        \"2.5 Training\",\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        summary_abstract,\n",
    "        summary_intro1 + summary_intro2,\n",
    "        summary_approach21,\n",
    "        summary_approach22,\n",
    "        summary_approach23,\n",
    "        summary_approach24,\n",
    "        summary_approach25,\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "173d81b2-7a7a-4f64-9c3c-f50d9d5d37e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      "\n",
      "\n",
      " A simple pre-training task is an efficient and scalable way to learn SOTA-like representations from scratch on a dataset (image, text) pairs collected from the internet . Natural language is used to reference learned visual concepts (or new ones) enabling zero-shot transfer of the model to downstream tasks .\n",
      "\n",
      "\n",
      "1 Introduction:\n",
      "\n",
      "\n",
      " Pre-training methods which learn directly from raw text have revolutionized NLP over the last few years . The development of \"text-to-text\" as a standardized input-output interface has enabled taskagnostic-based architectures to zero-shot transfer to downstream data . Using natural language supervision for image representation learning is still rare . This is because demonstrated performance on common benchmarks is much lower than alternative approaches . Instead, more narrowly scoped but well-targeted uses of weak supervision have improved performance .\n",
      "\n",
      "\n",
      "2.1 Natural Language Supervision:\n",
      "\n",
      "\n",
      " At the core of our approach is the idea of learning perception from supervision contained in natural language . Zhang et al. (2020), Gomez et. (2019), Gomez & Johnson (2020) all introduce methods that learn visual representations from text with images . We emphasize that what is common across this line of work is not any of the details of the particular methods used but the appreciation of natural language as a training signal .\n",
      "\n",
      "\n",
      "2.2 Creating a Sufficiently Large Dataset:\n",
      "\n",
      "\n",
      " Existing work has mainly used three datasets, MS-COCO, Visual Genome, and YFCC100M . The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2.2.\n",
      "\n",
      "\n",
      "2.3 Selecting an Efficient Pre-Training Method:\n",
      "\n",
      "\n",
      " We found training efficiency was key to successfully scaling natural language supervision . We train CLIP from scratch without initializing the image encoder or the text encoder with pre-trained weights . CLIP learns a high pointwise mutual information as well as the names of Wikipedia articles .\n",
      "\n",
      "\n",
      "2.4 Choosing and Scaling a Model:\n",
      "\n",
      "\n",
      " The image encoder is a Transformer (Vaswani et al., 2017) with a 49,152 vocab size . The text sequence isacketed with [SOS] and [EOS] tokens and the activations are treated as the feature representation of the text . Masked self-attention was used in the text encoder to preserve the ability to use a pre-trained language model .\n",
      "\n",
      "\n",
      "2.5 Training:\n",
      "\n",
      "\n",
      " We consider two different architectures for the image encoder . We use ResNet-50 (He et al., 2016a) and Vision Transformer(ViT) architecture . The text encoder is a Transformer (Vaswani et al. 2017) with 8 attention heads . Masked self-attention (SOS) tokens were used in the text encoders to preserve the ability to use a pre-trained language model or add language-based models .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print(row[\"section\"] + \":\\n\\n\")\n",
    "    print(row[\"summary\"] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57253217-47ba-4777-9c04-c035469460ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
